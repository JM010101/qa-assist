[
  {
  "question": "Have you built agents with Anthropic Agents SDK or migrated from custom orchestration to a framework? Share specifics.",
  "answer": "Yes. I’ve built tool-using agents using the Claude Agent SDK (formerly Claude Code SDK), leveraging its built-in agent loop, built-in tools (e.g., file I/O, command execution), and its context-management primitives for long-running episodes. In practice, that removed a large amount of custom glue code and reduced tool-contract drift because the SDK standardizes tool invocation patterns and the agent harness behavior. \n\n- Migration experience: Yes. I’ve migrated custom orchestrators to a framework-style harness (e.g., state-machine / graph execution) when failures were harness-level (non-convergence, tool retries, context growth) rather than model-level. My migration pattern is a “strangler” approach: (1) freeze evaluator + trace schema, (2) port existing tools 1:1 behind a compatibility layer with strict JSON schemas and validation, (3) run side-by-side on a golden subset for parity, (4) cut over the main loop once tool parity and trace parity are proven, then (5) iterate on compaction, caching, and convergence policies.\n\n- What I focus on in these migrations: deterministic replay (to debug without web drift), a single canonical tool surface (no duplicate browser APIs), caching/memoization of tool results, and code-enforced validators for final-answer format and evidence/verification requirements. The goal is fewer iterations per task and higher long-horizon completion, not prompt complexity. "
  },
  {
  "question": "Looking at our failure pattern (65 tasks struggling 10+ iterations before failing), what's your diagnosis beyond 'no persistent runtime'?",
  "answer": "Persistent Python state is a major root cause, but 10+ iteration failures typically indicate missing convergence controls plus tool brittleness. Specifically, I would expect a mix of:\n\n1) Stagnation without detection: the harness lacks a novelty/progress signal (e.g., hashing normalized tool calls and detecting repeated calls; tracking whether the set of unknowns is shrinking). Without this, the agent retries the same search/fetch/python steps hoping for a different outcome.\n\n2) Missing caching/memoization: repeated SERP calls, repeated downloads, repeated parsing of the same URL/PDF. This inflates iterations/cost and increases variance (web drift) across retries.\n\n3) Tool-contract ambiguity: exposing multiple overlapping browser/search APIs (or inconsistent schemas) increases invalid calls and partial failures, which the model then “retries” rather than repairing.\n\n4) Prompt-only quality gates: if “verify, cite, validate format, don’t loop” exists only in the system prompt, compliance will be non-deterministic. Those gates must be enforced in code: answer-schema validation, max retry budgets, forced strategy shift on stagnation, and explicit repair steps.\n\n5) Web access brittleness: bot blocks / interstitials create repeated failures that look like uncertainty; the agent keeps trying instead of switching approach. Architecture fix: search via API + fetch via extractor, browser automation only as fallback, plus detection of 403/captcha and domain-level backoff/blacklist.\n\nNet: the runtime persistence fix addresses multi-step computation correctness; the iteration pathology is addressed by convergence policies (novelty + progress scoring), caching, unified tool contracts, and code-level validators (not prompt text). "
  },
  {
  "question": "For the Level 3 performance (11.5%), what's your hypothesis on why difficulty scaling breaks so dramatically?",
  "answer": "GAIA Level 3 is explicitly the long-horizon regime: it can require arbitrarily long sequences of actions, any number of tools, and broad world access (whereas Level 1 is typically ≤5 steps and Level 2 is ~5–10 steps). \n\nMy hypothesis for the sharp collapse is compounding error + harness fragility:\n\n1) Compounding step reliability: Level 3 success is the product of many correct sub-decisions (plan → search → retrieve → extract → compute → verify → normalize answer). Even modest per-step failure rates lead to near-zero end-to-end success when you need 20–40 meaningful steps.\n\n2) State continuity becomes mandatory: Level 3 disproportionately includes multi-stage extraction + computation. Stateless python execution (or fragile sandboxing) forces recomputation and increases the chance of inconsistent intermediates.\n\n3) Web/search limitations dominate: if Playwright is frequently blocked, Level 3 tasks will either (a) loop on retries, or (b) accept lower-quality evidence, causing downstream extraction and reasoning failures.\n\n4) Context growth / missing compaction: long episodes accumulate tool outputs; without compaction + structured working memory (facts/open questions/intermediates with provenance), the agent loses the thread and restarts sub-plans. This is a known long-running agent failure mode, and it is typically solved by harness-level context management plus aggressive summarization of irrelevant tool outputs.\n\n5) Verification + normalization gaps: GAIA answers are unambiguous and evaluator sensitivity to formatting can turn “nearly correct” into fails. A code-enforced finalization step (type/format validation, unit/rounding checks, date normalization) converts many near-misses.\n\nSo the scaling break is expected given long-horizon requirements plus the specific architectural issues you observed (state loss, tool confusion, browsing brittleness, prompt-only gates). "
  }
  ]