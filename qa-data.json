[
  {
  "question": "Have you built agents with Anthropic Agents SDK or migrated from custom orchestration to a framework? Share specifics.",
  "answer": "Yes. I’ve built tool-using agents using the Claude Agent SDK (formerly Claude Code SDK), leveraging its built-in agent loop, built-in tools (e.g., file I/O, command execution), and its context-management primitives for long-running episodes. In practice, that removed a large amount of custom glue code and reduced tool-contract drift because the SDK standardizes tool invocation patterns and the agent harness behavior. \n\n- Migration experience: Yes. I’ve migrated custom orchestrators to a framework-style harness (e.g., state-machine / graph execution) when failures were harness-level (non-convergence, tool retries, context growth) rather than model-level. My migration pattern is a “strangler” approach: (1) freeze evaluator + trace schema, (2) port existing tools 1:1 behind a compatibility layer with strict JSON schemas and validation, (3) run side-by-side on a golden subset for parity, (4) cut over the main loop once tool parity and trace parity are proven, then (5) iterate on compaction, caching, and convergence policies.\n\n- What I focus on in these migrations: deterministic replay (to debug without web drift), a single canonical tool surface (no duplicate browser APIs), caching/memoization of tool results, and code-enforced validators for final-answer format and evidence/verification requirements. The goal is fewer iterations per task and higher long-horizon completion, not prompt complexity. "
  },
  {
  "question": "Looking at our failure pattern (65 tasks struggling 10+ iterations before failing), what's your diagnosis beyond 'no persistent runtime'?",
  "answer": "Persistent Python state is a major root cause, but 10+ iteration failures typically indicate missing convergence controls plus tool brittleness. Specifically, I would expect a mix of:\n\n1) Stagnation without detection: the harness lacks a novelty/progress signal (e.g., hashing normalized tool calls and detecting repeated calls; tracking whether the set of unknowns is shrinking). Without this, the agent retries the same search/fetch/python steps hoping for a different outcome.\n\n2) Missing caching/memoization: repeated SERP calls, repeated downloads, repeated parsing of the same URL/PDF. This inflates iterations/cost and increases variance (web drift) across retries.\n\n3) Tool-contract ambiguity: exposing multiple overlapping browser/search APIs (or inconsistent schemas) increases invalid calls and partial failures, which the model then “retries” rather than repairing.\n\n4) Prompt-only quality gates: if “verify, cite, validate format, don’t loop” exists only in the system prompt, compliance will be non-deterministic. Those gates must be enforced in code: answer-schema validation, max retry budgets, forced strategy shift on stagnation, and explicit repair steps.\n\n5) Web access brittleness: bot blocks / interstitials create repeated failures that look like uncertainty; the agent keeps trying instead of switching approach. Architecture fix: search via API + fetch via extractor, browser automation only as fallback, plus detection of 403/captcha and domain-level backoff/blacklist.\n\nNet: the runtime persistence fix addresses multi-step computation correctness; the iteration pathology is addressed by convergence policies (novelty + progress scoring), caching, unified tool contracts, and code-level validators (not prompt text). "
  },
  {
  "question": "For the Level 3 performance (11.5%), what's your hypothesis on why difficulty scaling breaks so dramatically?",
  "answer": "GAIA Level 3 is explicitly the long-horizon regime: it can require arbitrarily long sequences of actions, any number of tools, and broad world access (whereas Level 1 is typically ≤5 steps and Level 2 is ~5–10 steps). \n\nMy hypothesis for the sharp collapse is compounding error + harness fragility:\n\n1) Compounding step reliability: Level 3 success is the product of many correct sub-decisions (plan → search → retrieve → extract → compute → verify → normalize answer). Even modest per-step failure rates lead to near-zero end-to-end success when you need 20–40 meaningful steps.\n\n2) State continuity becomes mandatory: Level 3 disproportionately includes multi-stage extraction + computation. Stateless python execution (or fragile sandboxing) forces recomputation and increases the chance of inconsistent intermediates.\n\n3) Web/search limitations dominate: if Playwright is frequently blocked, Level 3 tasks will either (a) loop on retries, or (b) accept lower-quality evidence, causing downstream extraction and reasoning failures.\n\n4) Context growth / missing compaction: long episodes accumulate tool outputs; without compaction + structured working memory (facts/open questions/intermediates with provenance), the agent loses the thread and restarts sub-plans. This is a known long-running agent failure mode, and it is typically solved by harness-level context management plus aggressive summarization of irrelevant tool outputs.\n\n5) Verification + normalization gaps: GAIA answers are unambiguous and evaluator sensitivity to formatting can turn “nearly correct” into fails. A code-enforced finalization step (type/format validation, unit/rounding checks, date normalization) converts many near-misses.\n\nSo the scaling break is expected given long-horizon requirements plus the specific architectural issues you observed (state loss, tool confusion, browsing brittleness, prompt-only gates). "
  },
  {
    "question": "How will you reproduce our baseline GAIA score and ensure apples-to-apples comparisons?",
    "answer": "I will lock down a single, versioned evaluation command/config that runs the GAIA public validation set and emits a deterministic metrics bundle: overall + L1/L2/L3 accuracy, mean/median iterations, tool error rates, and cost. I will also freeze model/version, max-iteration budgets, tool settings, and normalization rules so we can attribute deltas to code changes rather than run variance. GAIA defines levels by step/tool requirements and Level 3 explicitly allows arbitrarily long action sequences, so baseline comparability is critical. "
    },
    {
    "question": "What will Milestone 1 deliver beyond a narrative review of traces?",
    "answer": "Milestone 1 output is a quantified failure model: (1) a labeled taxonomy for every failed episode (primary + secondary causes), (2) a Pareto chart by GAIA level (e.g., web-block vs state-loss vs extraction vs non-convergence), (3) a golden subset of representative tasks per category for regression testing, and (4) a rewrite decision memo (SDK migration vs incremental) with estimated lift per fix category. GAIA’s level definitions (L1 <= ~5 steps, L2 ~5–10 steps, L3 arbitrarily long sequences) inform prioritization. "
    },
    {
    "question": "How will you stop 10+ iteration thrash loops in a principled way?",
    "answer": "I implement convergence controls in the harness: (1) tool-call memoization (hash tool name + normalized args; reuse results), (2) novelty detection (block repeated calls that produce no new facts), (3) progress scoring (open-questions set must shrink or strategy changes), and (4) explicit fallback policies (e.g., if browser blocked, switch to API search + HTTP fetch). This turns retries from an emergent behavior into an explicit, bounded policy. "
    },
    {
    "question": "How will you implement persistent Python state across steps (and prove it works)?",
    "answer": "One Python kernel/session per GAIA episode: the orchestrator creates the session once, routes every python tool call to the same session, and tears it down at the end. I will add integration tests that span multiple tool calls (import once, define variables once, reuse later) and a health-check + restart path that rehydrates minimal state if the session dies. For E2B, I’ll use a template-based sandbox so the session runs in a stable prebuilt environment. "
    },
    {
    "question": "How do you avoid repeated package installs (pandas/requests/etc.) in traces?",
    "answer": "I will build an E2B sandbox template using an e2b.Dockerfile and reuse the resulting template ID for all episodes. This is exactly what E2B templates are for: preinstall common dependencies so runtime doesn’t waste iterations on pip installs. I will also add a harness gate: if the model attempts 'pip install' for a known preinstalled package, block and redirect to using the existing environment. "
    },
    {
    "question": "How will you handle template readiness and reduce cold-start flakiness?",
    "answer": "I will use a template 'ready command' (or equivalent) so the snapshot is only taken after readiness checks pass (imports succeed, services started, etc.). This eliminates intermittent 'not ready' errors that masquerade as reasoning failures. "
    },
    {
    "question": "What’s your browsing architecture given Playwright is being blocked?",
    "answer": "I separate search from retrieval: (1) search via an API-based search tool (stable SERP), (2) fetch via HTTP + extraction pipeline (readability/boilerplate removal + PDF parsing), and (3) browser automation only as a last resort for JS-heavy sites. Additionally, I implement block detection (403/captcha/interstitial classifiers) with domain backoff/blacklist so the agent doesn’t loop on the same blocked target. "
    },
    {
    "question": "How will you manage context growth in long-running (Level 3) episodes?",
    "answer": "I will enforce structured working memory (facts, open questions, plan, intermediate results with provenance) plus compaction of raw tool outputs. The Claude Agent SDK explicitly supports compaction for long-running agents by summarizing prior messages near context limits; if we stay custom, we replicate the same principle: keep only high-signal artifacts in the prompt and move everything else to external state. "
    },
    {
    "question": "What does an SDK migration look like without stalling delivery?",
    "answer": "Strangler migration: (1) freeze evaluator and trace schema, (2) port tools 1:1 with strict schemas, (3) run side-by-side on a golden subset until parity, (4) swap in the SDK agent loop, then (5) iterate on compaction, caching, and validation policies. The Agent SDK is explicitly positioned as providing the same agent loop and context management that powers Claude Code, which reduces bespoke harness risk under benchmark pressure. "
    },
    {
    "question": "How will you design and run evals during development (not just final GAIA runs)?",
    "answer": "I will implement a layered eval strategy: (1) unit tests for tool contracts and validators, (2) golden-subset regression tests (20–40 representative GAIA tasks by failure mode), and (3) nightly full GAIA runs. Anthropic’s eval guidance emphasizes automated evals that can be run during development, which aligns with this approach. "
    },
    {
    "question": "How do you secure an agent that can browse the web and execute code (prompt injection risk)?",
    "answer": "I implement defense-in-depth: (1) isolate execution (sandbox + least-privilege filesystem), (2) strict network egress controls and allowlists, (3) separate instructions from untrusted content (web text treated as data), (4) tool call approval policies for sensitive actions, and (5) logging + auditing of tool actions. This aligns with Anthropic’s guidance on securing Agent SDK deployments and their published research on prompt injection defenses; OWASP’s prompt injection prevention guidance provides additional concrete controls. "
    },
    {
    "question": "How will you eliminate system prompt contradictions and tool confusion (e.g., two browser APIs)?",
    "answer": "I reduce model-facing surface area: one canonical set of tool schemas (search, fetch, browser, python), schema validation at the dispatcher boundary, and removal of legacy/duplicate tool definitions from the model context. The system prompt becomes minimal (role + tool schemas + output constraints), while quality gates (format/evidence/verification/max retries) are enforced programmatically in the harness. "
    },
    {
    "question": "How will you improve Level 3 specifically, not just Level 1/2?",
    "answer": "Level 3 is the long-horizon regime (arbitrarily long action sequences). I focus on step reliability and compounding error: persistent runtime, robust search/fetch (avoid blocks), compaction/structured memory, extraction fallbacks (PDF/table handling), and enforced verification/normalization. This combination improves the probability of completing long chains without derailment, which is exactly what GAIA’s Level 3 definition stresses. "
    },
    {
    "question": "How will you handle tool integration at scale without custom one-offs?",
    "answer": "I prefer protocol-based integration where possible. MCP is an open standard introduced by Anthropic to connect tools/data sources to AI applications through a consistent interface, reducing fragmentation and duplicated integration effort. If MCP fits your ecosystem, we can standardize integrations and simplify tool discovery/maintenance. "
    },
    {
    "question": "What will your daily progress reporting look like (to avoid 'output dumps')?",
    "answer": "Daily deliverables are measurable: a diff of GAIA metrics (overall/L1/L2/L3), iteration distribution changes, tool error rates (403/captcha/timeouts), and a short changelog mapping each code change to the expected failure category reduction. Commits are small and reviewable (feature flags where needed), with golden-subset runs on every PR and a full GAIA run on a set cadence. "
    },
    {
      "question": "How will you reproduce our baseline GAIA score and ensure results are comparable run-to-run?",
      "answer": "I will lock a single versioned evaluation entrypoint (command + config) and freeze: model version, tool settings, max-iteration budgets, timeouts, and scoring/normalization rules. Then I will run the GAIA public validation set and store a metrics bundle (overall/L1/L2/L3, iteration distribution, tool error rates, cost). GAIA’s public validation set is commonly reported as 165 questions, so I will ensure we are running the same split and identical evaluator settings for apples-to-apples comparisons. "
      },
      {
      "question": "What’s your plan for Milestone 1 deliverables (what exactly will we get)?",
      "answer": "A quantified failure model, not a narrative: (1) per-task labeling into a failure taxonomy (primary + secondary cause), (2) Pareto charts by GAIA level, (3) a golden regression subset (20–40 tasks covering dominant failure modes), (4) a rewrite decision memo (Agent SDK migration vs incremental) with estimated impact per fix, and (5) an implementation backlog prioritized by expected L1/L2/L3 lift and engineering risk. "
      },
      {
      "question": "How do you diagnose the 65 tasks that loop 10+ iterations beyond 'no persistent runtime'?",
      "answer": "I treat 10+ iteration loops as convergence-control failure: repeated equivalent tool calls without novelty, missing caching/memoization, tool-contract confusion (duplicate browser APIs or schema drift), and prompt-only 'quality gates' not enforced by code. I’ll measure repeated-call rate, stagnation rate (no new facts added per iteration), and domain-level block rate; then implement novelty guards, caching, strategy-shift rules, and enforced validators to bound retries. "
      },
      {
      "question": "Why does Level 3 collapse so hard (11.5%) compared to Levels 1/2?",
      "answer": "GAIA Level 3 is the long-horizon regime: it can require arbitrarily long sequences of actions and any number of tools, and the paper notes Level 3 questions can take more than 40 steps. Small per-step failure probabilities compound across long trajectories, so harness reliability (state, tool robustness, context management, and convergence policies) dominates Level 3 outcomes. "
      },
      {
      "question": "How will you implement persistent Python state across steps?",
      "answer": "One Python session per GAIA episode (task): create a single kernel/sandbox at episode start, route every python tool call to that same session, persist files/artifacts, and destroy at episode end. Add health checks and a controlled restart that rehydrates minimal state from structured episode memory if the session dies. This is the minimum requirement for multi-step computation and artifact workflows. "
      },
      {
      "question": "How will you eliminate repeated package installs (pandas/requests/etc.)?",
      "answer": "Use a sandbox template with dependencies baked in (e.g., an E2B template via an e2b.Dockerfile and a reusable template ID), and add a harness guardrail to block redundant installs and redirect to preinstalled libs. This reduces latency, cost, and iteration thrash caused by 'pip install' loops. "
      },
      {
      "question": "What is your web browsing/search architecture given Playwright is getting flagged?",
      "answer": "Separate search from retrieval: (1) use an API-based search tool for stable SERP results, (2) use HTTP fetch + extraction (readability/boilerplate removal; PDF parsing) for most pages, (3) use browser automation only as a fallback for JS-heavy targets, plus detection of block signals (403/captcha/interstitial) with domain backoff/blacklist to prevent retry loops. "
      },
      {
      "question": "How do you handle PDFs, tables, and hard extraction tasks reliably?",
      "answer": "Layered extraction: try text extraction first, then table extraction fallback, then render/screenshot + OCR only when necessary. Store provenance (URL + extracted snippets/offsets) and run a verification pass that rechecks the extracted value before final answer formatting. This reduces 'found it but misread it' errors typical in multi-tool benchmarks. "
      },
      {
      "question": "How will you manage context growth and memory for long-running episodes?",
      "answer": "Structured working memory outside the prompt (facts, open questions, plan, intermediate resultsa intermediates with provenance) plus compaction of raw tool outputs as context approaches limits. If we migrate to Anthropic’s Agent SDK, we can leverage its compaction feature for long-running agents; if we stay custom, we replicate the same principle: keep only high-signal state in context and push raw artifacts to external storage/state. "
      },
      {
      "question": "Do you recommend an architectural rewrite to the Anthropic Agent SDK or incremental fixes first?",
      "answer": "I’ll decide after Milestone 1 based on trace evidence. If dominant failures are harness-level (context management, tool loop correctness, compaction, stagnation), migrating to the Agent SDK reduces bespoke orchestration risk and aligns with Anthropic’s recommended long-running agent patterns. If failures are localized (stateless python executor, browsing stack, tool schema mismatch), incremental fixes can deliver faster uplift in a week. "
      },
      {
      "question": "If we migrate to the Agent SDK, what’s your migration plan to avoid downtime?",
      "answer": "Strangler approach: freeze evaluator + trace schema; port tools 1:1 with strict schemas and validation; run side-by-side on a golden subset to reach parity; then swap the core loop; finally optimize compaction/caching/validators. This keeps scoring and observability stable while the harness is replaced. "
      },
      {
      "question": "How will you ensure tool schemas are correct and avoid tool-contract drift?",
      "answer": "Single canonical tool surface (search, fetch, browser, python) with strict JSON schema validation at the dispatcher boundary. If the model emits invalid args, the harness returns a structured schema error and triggers a repair step. I also add integration tests to ensure the model-facing tool spec matches the runtime dispatcher. "
      },
      {
      "question": "How do you move 'quality gates' out of the prompt and into code?",
      "answer": "Implement programmatic validators: final answer type/format validation, unit/rounding/date normalization checks, evidence requirements for web-derived claims, and a mandatory verification pass for multi-step computations. Prompt becomes minimal (role + tool schemas + output constraints); enforcement happens in code. "
      },
      {
      "question": "How will you reduce context usage and improve long-horizon performance beyond compaction?",
      "answer": "Use programmatic tool calling and tool search/discovery patterns so tool invocation and tool metadata do not unnecessarily inflate the conversational context. Anthropic describes programmatic tool calling as a way to invoke tools in a code execution environment and reduce impact on the model’s context window. "
      },
      {
      "question": "What is your plan for deterministic debugging given web drift and non-determinism?",
      "answer": "Record/replay: persist tool inputs/outputs (search results, fetch payloads, parsed text, python outputs) and provide a replay mode that bypasses live web. Use replay for PR validation and regression debugging; use live mode for final GAIA scoring. This avoids chasing phantom regressions caused by changing web results. "
      },
      {
      "question": "How will you quantify improvements besides accuracy?",
      "answer": "In addition to overall/L1/L2/L3 accuracy, I will track: mean/median iterations per episode, repeated tool-call rate, blocked-domain rate, tool error rate (timeouts/403/captcha), time-to-first-useful-fact, and cost per solved task. These are leading indicators that typically move before accuracy fully catches up. "
      },
      {
      "question": "How will you select iteration budgets and stop conditions to avoid cost blowups?",
      "answer": "Set level-aware iteration budgets and enforce stagnation-based early stopping: if novelty/progress metrics show no new facts after N iterations, force strategy shift; cap per-domain retries; cap tool call budgets; and require a finalization step with verification/normalization before termination. This converts unbounded thrash into bounded policy. "
      },
      {
      "question": "How will you address prompt injection risk when the agent browses the open web?",
      "answer": "Treat all external content as untrusted data: separate instructions from retrieved content, validate outputs against schemas, restrict tool permissions (least privilege), implement allowlists/egress controls, and log/audit tool actions. OWASP’s prompt injection guidance and OWASP’s GenAI risk taxonomy emphasize prompt injection as a top risk and recommend layered mitigations and validation. "
      },
      {
      "question": "How will you harden the system against the model hallucinating tool outputs or citations?",
      "answer": "Enforce provenance: tool results are stored as structured artifacts and the model must reference artifact IDs/snippets in a verifier step. If the final answer cannot be supported by recorded tool artifacts, the validator fails and triggers a repair path. This prevents 'fabricated evidence' from passing through the harness. "
      },
      {
      "question": "How will you handle multiple overlapping browser APIs currently in the prompt/tooling?",
      "answer": "I will collapse to one browser tool and one fetch tool with clear responsibilities, remove the duplicate/legacy APIs from the model-facing tool list, and update the system prompt accordingly. Duplicate APIs increase schema confusion and invalid calls, which is a known driver of long retry loops. "
      },
      {
      "question": "What is your approach to tool writing and tool descriptions so the model uses them correctly?",
      "answer": "Treat tool design as product design: crisp descriptions, explicit args/returns, failure modes, and examples. Anthropic emphasizes that tool descriptions/specs are a primary lever for improving whether agents call tools correctly and for diagnosing miscalls. "
      },
      {
      "question": "What access and artifacts do you need from us on day one?",
      "answer": "Repo access (or full source drop), the exact GAIA evaluation command/config used for the baseline, the evaluator/scoring rules, trace storage access (viewer + raw JSONL), and credentials for model + sandbox + search/fetch providers. Without matching the baseline run, we cannot attribute improvements to changes reliably. "
      },
      {
      "question": "How will you structure daily progress so it’s not 'LLM output dumps'?",
      "answer": "Daily commits with small, reviewable diffs; a daily metrics delta (overall/L1/L2/L3 plus iteration/tool-error changes); and a short changelog mapping each change to a failure category. I will also maintain a golden subset and run it per-PR, with full GAIA runs on a fixed cadence to prevent regressions. "
      },
      {
      "question": "How will you explain realistic expectations on the target scores, especially for Level 3?",
      "answer": "I will treat targets as outcome goals but manage the work via leading indicators (iteration reduction, tool reliability, extraction correctness, and compaction effectiveness) because Level 3 performance is dominated by long-horizon reliability per GAIA’s definition. We should see iteration and tool-error improvements first, then accuracy lifts as compounding failure probability drops. "
      }
  ]